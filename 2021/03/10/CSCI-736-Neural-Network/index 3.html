<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="CSCI 736 Neural Network[toc] Paper Time Line Write 1 and 5     Paper Content Time Schedule           Paper Math Equation \begin{aligned} &amp;p_{end,j}&#x3D;\dfrac{e^{z_{j}}}{\sum{j&#x3D;n+1}^{2n} e^{z_{j}}} , j \i">
<meta property="og:type" content="article">
<meta property="og:title" content="CSCI 736 Neural Network">
<meta property="og:url" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="CSCI 736 Neural Network[toc] Paper Time Line Write 1 and 5     Paper Content Time Schedule           Paper Math Equation \begin{aligned} &amp;p_{end,j}&#x3D;\dfrac{e^{z_{j}}}{\sum{j&#x3D;n+1}^{2n} e^{z_{j}}} , j \i">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/5871614469525_.pic_hd.jpg">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/5991614469878_.pic_hd.jpg">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/v2-3884f344d71e92d70ec3c44d2795141f_1440w-20210308084120851.jpg">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/03/10/CSCI-736-Neural-Network/v2-b0175ebd3419f9a11a3d0d8b00e28675_1440w-20210308084146454.jpg">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/03/10/CSCI-736-Neural-Network/v2-9e50e23bd3dff0d91b0198d0e6b6429a_1440w-20210308084225295.jpg">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/image-20210309201413384.png">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/image-20210309202019613.png">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/image-20210309210413591.png">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/image-20210309220008147.png">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/image-20210309215606227.png">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/image-20210309215051149.png">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/image-20210309222925667.png">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/image-20210309223058277.png">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/image-20210310001401154.png">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/03/10/CSCI-736-Neural-Network/image-20210310001750278.png">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/image-20210309225757217.png">
<meta property="og:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/03/10/CSCI-736-Neural-Network/image-20210310002011618.png">
<meta property="article:published_time" content="2021-03-10T06:39:36.000Z">
<meta property="article:modified_time" content="2021-03-10T07:37:49.354Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/03/10/CSCI-736-Neural-Network/5871614469525_.pic_hd.jpg">

<link rel="canonical" href="http://example.com/2021/03/10/CSCI-736-Neural-Network/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>CSCI 736 Neural Network | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/03/10/CSCI-736-Neural-Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CSCI 736 Neural Network
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-03-10 01:39:36 / Modified: 02:37:49" itemprop="dateCreated datePublished" datetime="2021-03-10T01:39:36-05:00">2021-03-10</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="CSCI-736-Neural-Network"><a href="#CSCI-736-Neural-Network" class="headerlink" title="CSCI 736 Neural Network"></a>CSCI 736 Neural Network</h1><p>[toc]</p>
<h1 id="Paper-Time-Line"><a href="#Paper-Time-Line" class="headerlink" title="Paper Time Line"></a>Paper Time Line</h1><p> Write 1 and 5</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Paper Content</th>
<th>Time Schedule</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="/2021/03/10/CSCI-736-Neural-Network/5871614469525_.pic_hd.jpg" alt="5871614469525_.pic_hd" style="zoom: 25%;"></td>
<td><img src="/2021/03/10/CSCI-736-Neural-Network/5991614469878_.pic_hd.jpg" alt="5991614469878_.pic_hd" style="zoom:25%;"></td>
</tr>
</tbody>
</table>
</div>
<h2 id="Paper-Math-Equation"><a href="#Paper-Math-Equation" class="headerlink" title="Paper Math Equation"></a>Paper Math Equation</h2><script type="math/tex; mode=display">
\begin{aligned}
&p_{end,j}=\dfrac{e^{z_{j}}}{\sum{j=n+1}^{2n} e^{z_{j}}} , j \in\{n+1, n+2, n+3 \ldots 2 n\}\\
&input=[q_{1},q_{2},q_{3},\cdots, q_{N}],[d_{1}, d_{2}, d_{3}, \cdots,d_{M}], \text{(N,M : the number of tokens)}\\
&outputs=[index\ \ of\ \ max(p_{i}),index\ \ of\ \  max(p_{j}))]\\

&p_{start}=\frac{e^{z_i}}{\sum\limits_{i=1}^{n}e^{z_i}}, i \in \{1,2,3\cdots, M\}\\

&p_{end}=\frac{e^{z_{j}}}{\sum\limits_{j=1}^{n} e^{z_{j}}},\\ &j \in\{1, 2, 3 \ldots M\} \\

&loss=-log(p_{start,I})-log(p_{end,J})\\
&\text{the correct start and end position are }\\&
I \in\{1,2,3,4 \ldots \mathrm{n},J \in\{1,2,3,4 \ldots \mathrm{n}\}


\end{aligned}</script><h2 id="Learning-Resource"><a href="#Learning-Resource" class="headerlink" title="Learning Resource"></a>Learning Resource</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1JE411g7XF?from=search&amp;seid=13732374700367344665">李宏毅</a></p>
<h1 id="文章理解"><a href="#文章理解" class="headerlink" title="文章理解"></a>文章理解</h1><p>embedding(八种常用的embedding)-&gt;rnn-&gt;lstm&amp;gru-&gt;attention-&gt;seq2seq-&gt;self-attention-&gt;transformer-&gt;bert</p>
<p><strong>embedding</strong></p>
<p><strong>RNN</strong></p>
<p><img src="/2021/03/10/CSCI-736-Neural-Network/v2-3884f344d71e92d70ec3c44d2795141f_1440w-20210308084120851.jpg" alt="v2-3884f344d71e92d70ec3c44d2795141f_1440w" style="zoom:25%;"></p>
<p><img src="/2021/03/10/CSCI-736-Neural-Network/03/10/CSCI-736-Neural-Network/v2-b0175ebd3419f9a11a3d0d8b00e28675_1440w-20210308084146454.jpg" alt></p>
<p><img src="/2021/03/10/CSCI-736-Neural-Network/03/10/CSCI-736-Neural-Network/v2-9e50e23bd3dff0d91b0198d0e6b6429a_1440w-20210308084225295.jpg" alt></p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a><strong>LSTM</strong></h3><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a><strong>GRU</strong></h3><h3 id="attention"><a href="#attention" class="headerlink" title="attention"></a><strong>attention</strong></h3><p>参数少, 速度快, 效果好</p>
<p><a target="_blank" rel="noopener" href="https://shangzhih.github.io/jian-shu-attentionji-zhi.html">优质教程</a></p>
<h3 id="Encoder-amp-Decoder"><a href="#Encoder-amp-Decoder" class="headerlink" title="Encoder&amp;Decoder"></a><strong>Encoder&amp;Decoder</strong></h3><p>一类算法的统称</p>
<p>这类算法的统称:</p>
<ol>
<li>无论输入和输出的长度是什么, 中间的 向量c 长度固定</li>
<li>根据不同任务可以选择不同的编码器和解码器</li>
</ol>
<p>缺点: 当输入信息太长时，会丢失掉一些信息.</p>
<h3 id="seq2seq-in-Encoder-amp-Deconder"><a href="#seq2seq-in-Encoder-amp-Deconder" class="headerlink" title="seq2seq $\in Encoder\&amp;Deconder$"></a><strong>seq2seq $\in Encoder\&amp;Deconder$</strong></h3><p>一类算法的统称</p>
<p>这类算法的统称:    满足输入序列, 输出序列的目的</p>
<h3 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a><strong>self-attention</strong></h3><h3 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a><strong>transformer</strong></h3><h3 id="bert"><a href="#bert" class="headerlink" title="bert"></a><strong>bert</strong></h3><p>linear classifier: two vector</p>
<p>each vector dot product the embedding, then apply softmax, find maximum to get index</p>
<h3 id="Deep-Auto-encoder"><a href="#Deep-Auto-encoder" class="headerlink" title="Deep Auto-encoder"></a><strong>Deep Auto-encoder</strong></h3><p>Paper:<a href="Papers/Deep  Auto-Encoder  Neural  Networks  in  Reinforcement  Learning.pdf">Deep  Auto-Encoder  Neural  Networks  in  Reinforcement  Learning</a></p>
<p>PPT:<a href="李宏毅PPT/Unsupervised Learning-Auto-encoder.pptx">Unsupervised Learning-Auto-encoder</a></p>
<p><img src="/2021/03/10/CSCI-736-Neural-Network/image-20210309201413384.png" alt="image-20210309201413384" style="zoom:50%;"></p>
<p>Stating from PCA</p>
<p>  <img src="/2021/03/10/CSCI-736-Neural-Network/image-20210309202019613.png" alt="image-20210309202019613" style="zoom:50%;"></p>
<p>可以把PCA的前半部分视为encode, 后半部分decode</p>
<p><strong>重要特性:增强robust</strong></p>
<p><img src="/2021/03/10/CSCI-736-Neural-Network/image-20210309210413591.png" alt="image-20210309210413591" style="zoom:50%;"></p>
<p>对CNN建立decoder, decoding的过程实际还是在卷积</p>
<p><img src="/2021/03/10/CSCI-736-Neural-Network/image-20210309220008147.png" alt="image-20210309220008147" style="zoom:50%;"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>图一</th>
<th>图二</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="/2021/03/10/CSCI-736-Neural-Network/image-20210309215606227.png" alt="image-20210309215606227" style="zoom:50%;"></td>
<td><img src="/2021/03/10/CSCI-736-Neural-Network/image-20210309215051149.png" alt="image-20210309215051149" style="zoom:50%;"></td>
</tr>
</tbody>
</table>
</div>
<p>这一页ppt只是简单讲了下如何世纪一个discrminator 来保证auto-encoder decoder效果好(decoder尽可能把vector还原为原始图像)</p>
<p>所以这启发我们: 我们需要一个足够好的能够衡量encoder与decoder的discriminator/classifier来监督encoder与decoder的训练与一个足够好的encoder来保证vector与原始图像能尽量一一对应(每个原始数据尽量能有独一无二的embedding)</p>
<p>参考文章: Deep InfoMax (DIM)</p>
<p>若训练集是sequential, skip thought</p>
<h2 id="skip-thought-gt-quick-thought-https-arxiv-org-pdf-1803-02893-pdf"><a href="#skip-thought-gt-quick-thought-https-arxiv-org-pdf-1803-02893-pdf" class="headerlink" title="skip thought-&gt;quick thought https://arxiv.org/pdf/1803.02893.pdf"></a>skip thought-&gt;quick thought <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1803.02893.pdf">https://arxiv.org/pdf/1803.02893.pdf</a></h2><p>quick thought 只认encoder不管decoder, 每一个句子的ebedding跟他下一个句子的embedding越接近越好, 跟随机的句子的emberdding越不同越好</p>
<p>quick设计的classifier:输入句子A应用encoder产生的embedding, 句子A的下一句应用encoder产生的embedding, 一对随机句子应用encoder产生的embedding, classifier需要能够认为句子A的下一句巨句子A的相似度最高</p>
<p>这样的classifier与产生这个embedding的encoder同时训练</p>
<h2 id="Feature-Disentangle"><a href="#Feature-Disentangle" class="headerlink" title="Feature Disentangle"></a>Feature Disentangle</h2><p><img src="/2021/03/10/CSCI-736-Neural-Network/image-20210309222925667.png" alt="image-20210309222925667" style="zoom:50%;"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>-</th>
<th>-</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="/2021/03/10/CSCI-736-Neural-Network/image-20210309223058277.png" alt="image-20210309223058277" style="zoom:50%;"></td>
<td><img src="/2021/03/10/CSCI-736-Neural-Network/image-20210310001401154.png" alt="image-20210310001401154" style="zoom:50%;"></td>
</tr>
</tbody>
</table>
</div>
<p>假设encoder返回的前100个embedding 放入speaker classifier中进行训练 ,直到speaker classifier无法区分出那种音色, 这时候就认为前100已经没有了音色信息, 音色信息跑到了后100个中</p>
<p>Instance normalization:一种特逼得layer, 可以抹掉不想要的信息</p>
<p>比如全部抹掉音色信息 ,那么剩下的就是纯正的语音信息, 具体方案依赖Gan实现</p>
<h2 id="Vector-Quantized-Variational-Auto-encoder-VQVAE"><a href="#Vector-Quantized-Variational-Auto-encoder-VQVAE" class="headerlink" title="Vector Quantized Variational Auto-encoder (VQVAE)"></a>Vector Quantized Variational Auto-encoder (VQVAE)</h2><div class="table-container">
<table>
<thead>
<tr>
<th>-</th>
<th>-</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="/2021/03/10/CSCI-736-Neural-Network/03/10/CSCI-736-Neural-Network/image-20210310001750278.png" alt="image-20210310001750278"></td>
<td><img src="/2021/03/10/CSCI-736-Neural-Network/image-20210309225757217.png" alt="image-20210309225757217" style="zoom:100%;"></td>
</tr>
</tbody>
</table>
</div>
<p>返回的vector对其内容做one-hot(最大的变1其余0)转换或者binary转换(设定threshold, 大于的变为1其余0), 推荐binary, 这样可以意外的发现训练集中原本不存在的cluster</p>
<p>假设codebook中只有5个vector, 则encoding返回的vector与这五个做相似度比较, 与codebook中哪一个相似就把codebook中的哪个返回给decoder</p>
<h2 id="seq2seq2seq"><a href="#seq2seq2seq" class="headerlink" title="seq2seq2seq"></a>seq2seq2seq</h2><p><img src="/2021/03/10/CSCI-736-Neural-Network/03/10/CSCI-736-Neural-Network/image-20210310002011618.png" alt="image-20210310002011618"></p>
<h2 id="Presentation"><a href="#Presentation" class="headerlink" title="Presentation"></a>Presentation</h2><ol>
<li><p>自我介绍, 标题页</p>
</li>
<li><p>第二页!!</p>
<p>the thoughts of our algorithm is  that you input the question and document and it return subspan of the documents as the answer.</p>
<p>Here is an example of the model</p>
</li>
<li><p>As we choose bert as baseline, we inputs tokens and bert return the answer’s start and end index.</p>
<p>So  let’s assume that our question has n tokens and documents have m tokens, we inputs their concatenation into bert.</p>
</li>
</ol>
<p>   最后一页!!!</p>
<p>   The bert will return vector $\C$ which has the same dismension as inputs but we only need the document part, because the answer is just the sub span of input document.</p>
<p>   As we need to find the start and end index, we prepare two linear classifiers</p>
<p>   We use them take dot product with $\C$’s document part and apply softmax to get p{start,i }and p{end,j} probabilty distribution</p>
<p>   Assume the correct start and end index named “I” and “J”, then we could get the p_I from p_startdistri, p_J from p_end_j distri ,then the loss fucntion for this sample is </p>
<script type="math/tex; mode=display">
   \text{should have correspond equation}</script><p>   Next, we apply backpropagation.</p>
<p>   we reapeat these processes until finish all training part.</p>
<p>   Beyond the basic QA system, our goal is to make it robust on unseen domain, to achieve such goal,we will try some existing strategies which will be discussed in related work and see if we can make any improvements.</p>
<p>CLS: the key word of the classifier</p>
<p>SEP: separate question tokens and document tokens</p>
<p>问题集</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/03/10/hello-world/" rel="prev" title="Hello World">
      <i class="fa fa-chevron-left"></i> Hello World
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/03/10/hexo%E6%8F%92%E4%BB%B6%E6%8C%87%E5%AF%BC/" rel="next" title="hexo插件指导">
      hexo插件指导 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#CSCI-736-Neural-Network"><span class="nav-number">1.</span> <span class="nav-text">CSCI 736 Neural Network</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper-Time-Line"><span class="nav-number">2.</span> <span class="nav-text">Paper Time Line</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Paper-Math-Equation"><span class="nav-number">2.1.</span> <span class="nav-text">Paper Math Equation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Learning-Resource"><span class="nav-number">2.2.</span> <span class="nav-text">Learning Resource</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%96%87%E7%AB%A0%E7%90%86%E8%A7%A3"><span class="nav-number">3.</span> <span class="nav-text">文章理解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM"><span class="nav-number">3.0.1.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GRU"><span class="nav-number">3.0.2.</span> <span class="nav-text">GRU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention"><span class="nav-number">3.0.3.</span> <span class="nav-text">attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder-amp-Decoder"><span class="nav-number">3.0.4.</span> <span class="nav-text">Encoder&amp;Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#seq2seq-in-Encoder-amp-Deconder"><span class="nav-number">3.0.5.</span> <span class="nav-text">seq2seq $\in Encoder\&amp;Deconder$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention"><span class="nav-number">3.0.6.</span> <span class="nav-text">self-attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformer"><span class="nav-number">3.0.7.</span> <span class="nav-text">transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bert"><span class="nav-number">3.0.8.</span> <span class="nav-text">bert</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Auto-encoder"><span class="nav-number">3.0.9.</span> <span class="nav-text">Deep Auto-encoder</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#skip-thought-gt-quick-thought-https-arxiv-org-pdf-1803-02893-pdf"><span class="nav-number">3.1.</span> <span class="nav-text">skip thought-&gt;quick thought https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1803.02893.pdf</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Feature-Disentangle"><span class="nav-number">3.2.</span> <span class="nav-text">Feature Disentangle</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vector-Quantized-Variational-Auto-encoder-VQVAE"><span class="nav-number">3.3.</span> <span class="nav-text">Vector Quantized Variational Auto-encoder (VQVAE)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#seq2seq2seq"><span class="nav-number">3.4.</span> <span class="nav-text">seq2seq2seq</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Presentation"><span class="nav-number">3.5.</span> <span class="nav-text">Presentation</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
